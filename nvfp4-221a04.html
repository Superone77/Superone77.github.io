<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📖&lt;/text&gt;&lt;/svg&gt;">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>NVFP4：高效精准的低精度推理新格式&nbsp;|&nbsp;Vinci’s Garden</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="NVFP4：高效精准的低精度推理新格式">
  
    <meta name="description" content="NVFP4 不仅是一种低精度格式，更是在模型智能保持方面迈出重要一步的压缩技术。">
    <meta property="og:description" content="NVFP4 不仅是一种低精度格式，更是在模型智能保持方面迈出重要一步的压缩技术。">
  
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📖&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;😀&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About Vinci</span>
        </div>
      </a>
    
  
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
    <h1 class="Header__Title">NVFP4：高效精准的低精度推理新格式</h1>
    
      <div class="DateTagBar">
        
          <span class="DateTagBar__Item DateTagBar__Date">Posted on Sun, Jun 29, 2025</span>
        
        
          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--brown">
            <a href="tag/Quantization.html">Quantization</a>
          </span>
        
      </div>
    
  </header>
  <article id="https://www.notion.so/221a0479abc78067a93ef2e41cb76887" class="PageRoot"><blockquote id="https://www.notion.so/221a0479abc780b18ac3d47817cf634a" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/">https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/</a></span></span></blockquote><div id="https://www.notion.so/221a0479abc780b48d27c4e427386133" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">为了充分发挥人工智能的潜力，优化至关重要。在开发者考虑推理阶段对 AI 模型进行优化时，常见的模型压缩技术包括量化、蒸馏和剪枝，其中最常用且影响最广的是量化。这主要得益于其在特定任务下的后处理精度表现，以及其所支持的框架和技术选择较为丰富。</span></span></p></div><div id="https://www.notion.so/221a0479abc78020a65ecda5bd61bd98" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">然而，模型量化的主要挑战在于可能造成模型智能或任务特定精度的下降，尤其是在从较高精度的数据类型（如 FP32）过渡到最新的 FP4 格式时。NVIDIA Blackwell 提供了最大限度的灵活性，支持 FP64、FP32/TF32、FP16/BF16、INT8/FP8、FP6 及 FP4 等多种数据格式。图 1 展示了 NVIDIA Ampere、Hopper 与 Blackwell GPU 所支持的最小浮点数据类型及其对应的稠密/稀疏性能，对比了各代 GPU 在性能与数据类型支持方面的演进。</span></span></p></div><div id="https://www.notion.so/221a0479abc78014a92df5af87489f36" class="Image Image--PageWidth"><figure><a href="attachment:fc6d2b52-bd6a-4399-a605-698b3a6679f6:image.png?width=809.96875"><img src="attachment:fc6d2b52-bd6a-4399-a605-698b3a6679f6:image.png?width=809.96875" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/221a0479abc780cea49ae9f560972171" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">最新的第五代 NVIDIA Blackwell 张量核心为多种超低精度格式的发展奠定了基础，既支持前沿研究，也适用于实际应用。表 1 对比了 NVIDIA Blackwell 所支持的三种主要 4 位浮点格式——FP4、MXFP4 和 NVFP4，重点展示了它们在结构、内存占用和精度方面的关键差异。该表说明了 NVFP4 如何在继承早期格式简洁性的基础上，兼顾模型精度的保持。</span></span></p></div><div id="https://www.notion.so/221a0479abc7809c8f81e8610db1b8dc" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">表 1. Blackwell 支持的 4 位浮点格式对比</span></span></p></div><div><div></div><div></div><div></div><div></div><div></div></div><div id="https://www.notion.so/221a0479abc78087a206e46dc81d7d9b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">本文介绍了 NVFP4——一种先进的数据类型，并说明了其如何专为 Blackwell 架构设计，以在超低精度下实现更高的推理精度与更高效的扩展能力。</span></span></p></div><h1 id="https://www.notion.so/221a0479abc780a8bb42de36e6d366a6" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/221a0479abc780a8bb42de36e6d366a6"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">什么是 NVFP4？</span></span></h1><div id="https://www.notion.so/221a0479abc78045a72ee2e7580b5a7b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">NVFP4 是在 NVIDIA Blackwell GPU 架构中引入的一种创新型 4 位浮点格式。它基于低比特“微型”浮点格式的概念，并通过提供额外的格式选项，为开发者带来了更大的灵活性。</span></span></p></div><div id="https://www.notion.so/221a0479abc780b0af02fb1e3175904c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">NVFP4 的结构与多数 4 位浮点格式（E2M1）相似，即包括 1 位符号位、2 位指数位和 1 位尾数位，其数值范围大致在 -6 到 6 之间。例如，该格式可能表示的数值包括：0.0、0.5、1.0、1.5、2、3、4、6（负值部分亦同）。</span></span></p></div><div id="https://www.notion.so/221a0479abc78014a838e880e609e04c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在超低精度格式中，保持数值在广泛动态范围内的精度是一项关键挑战。NVFP4 针对该问题引入了两项架构创新，使其在 AI 推理中表现出色：</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/221a0479abc7800a8119e5fa1242a3e9" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">高精度缩放因子编码</strong></span></span></li><li id="https://www.notion.so/221a0479abc780f0904fef501da3c728" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">两级微块缩放策略</strong></span></span></li></ul><div id="https://www.notion.so/221a0479abc780ef838dec60632f344a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">该策略对每个包含 16 个值的微块（micro-block）应用精细的 E4M3 缩放因子，同时对整个张量施加第二级 FP32 缩放因子。这两级缩放机制协同工作，能更精确地表示数值，并显著降低量化误差（见图 2）。</span></span></p></div><div id="https://www.notion.so/221a0479abc78093a856f918d90a8336" class="Image Image--Normal"><figure><a href="attachment:797ac780-8123-4edc-9f4f-84998b5f8dba:image.png?width=1248"><img src="attachment:797ac780-8123-4edc-9f4f-84998b5f8dba:image.png?width=1248" style="width:1248px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h1 id="https://www.notion.so/221a0479abc7803ebcefd33233a74dcc" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/221a0479abc7803ebcefd33233a74dcc"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">高精度缩放：编码更多信号，减少误差</strong></span></span></h1><div id="https://www.notion.so/221a0479abc78092a7ccc4ff04ce83ea" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">为了更有效地利用共享微块缩放机制，NVFP4 使用 E4M3 的 FP8 精度对数据块进行编码。与传统的 2 的幂次缩放不同，E4M3 格式支持带有小数精度的非幂次缩放因子，从而为张量的实际分布提供了更灵活、精确的表达能力。图 3 展示了一个全精度输入矩阵与使用 E8M0 和 E4M3 缩放后所得到的量化矩阵之间的对比。</span></span></p></div><div id="https://www.notion.so/221a0479abc7809aa213f2e496c67174" class="Image Image--PageWidth"><figure><a href="attachment:a3051973-01c2-4641-88d3-0fb08fc55f9c:image.png?width=809.984375"><img src="attachment:a3051973-01c2-4641-88d3-0fb08fc55f9c:image.png?width=809.984375" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/221a0479abc7802889a7f16497101b33" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">该图将全精度输入值与两种量化方案下的结果进行比较：E8M0（用于 MXFP4）和 E4M3（用于 NVFP4）。第一行展示的是使用幂次缩放（E8M0）后较为粗糙的量化值，而第二行展示的是使用更细粒度 E4M3 小数缩放后的 NVFP4 结果。NVFP4 能更贴近原始值，通过为每个微块选择更精确的共享缩放因子，从而减少误差。这说明 NVFP4 能在低比特推理中保留更多数值精度。</span></span></p></div><div id="https://www.notion.so/221a0479abc7809b86d7f9be721d2ebd" class="Image Image--PageWidth"><figure><a href="attachment:c5f70be2-2039-488e-b781-62e578e090cb:image.png?width=809.984375"><img src="attachment:c5f70be2-2039-488e-b781-62e578e090cb:image.png?width=809.984375" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/221a0479abc7809eb271df1616884dc9" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">E4M3 缩放因子的一个缺点是其数值范围相对较小。为解决这一问题，NVFP4 引入了第二级缩放机制：在张量层面应用 FP32 缩放因子（见图 2），以调节原始张量的分布，使得每个微块在使用 E4M3 缩放时更为有效，从而进一步提高整体量化精度。</span></span></p></div><div id="https://www.notion.so/221a0479abc7805fabc8c8ba6e129f01" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">图 4 的动画是图 3 中矩阵转换过程的数轴表示，展示了将原始全精度数值（以黄色圆圈表示）映射到量化数据类型动态范围中的对应位置。该过程的评估指标是从原始值到其量化表示的</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">平均均方误差（MSE）</strong></span><span class="SemanticString">，MSE 越低越好，其中 E4M3 的平均 MSE 为 0.08。</span></span></p></div><div id="https://www.notion.so/221a0479abc780b78d42dcf55da1393f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">E4M3 之所以“平均表现更好”，在于它选取的是一个</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">最优的小数缩放因子</strong></span><span class="SemanticString">，使得对微块中 16 个值的平方误差（或绝对误差）求和后，总体误差通常小于使用 E8M0 量化的情况。换句话说：</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/221a0479abc7809481aff74bbdf7cd7a" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">E8M0</strong></span><span class="SemanticString">：将缩放因子固定为最接近的 2ⁿ 值，这可能导致对块最大值（amax）的量化误差较大，从而使整个块的量化误差也增大。</span></span></li><li id="https://www.notion.so/221a0479abc78056ac86eaec0511ff01" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">E4M3</strong></span><span class="SemanticString">：寻找一个最优缩放因子，使整个微块的误差总和最小。虽然个别值可能稍微不够准确，但整体保留了更高的数值保真度，尤其是在表示块最大值（amax）时通常更精确。</span></span></li></ul><div id="https://www.notion.so/221a0479abc78046b14fd8b2090866a1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">你可能会问：既然 E4M3 更精确，为什么还要用 E8M0？答案是：</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">当简洁性优先时</strong></span><span class="SemanticString">。E8M0 的缩放因子具有更低的计算复杂度，不需要额外的每张量软件缩放因子，因此适用于对缩放精度不敏感的激活值或权重。而 E4M3 能根据每个小块的实际数值范围灵活调整缩放因子，使其在更广泛输入范围下都能提供更优的拟合效果。这种灵活性正是 NVFP4 在 4 位量化中实现更低舍入误差和更好模型精度保留的关键。</span></span></p></div><div id="https://www.notion.so/221a0479abc780baa879d736e52f6c63" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">NVIDIA Blackwell 第五代张量核心架构原生支持 NVFP4，并可自动处理微缩放 FP4 数据，包括元素分组、动态缩放及 4 位矩阵运算。</span></span></p></div><div id="https://www.notion.so/221a0479abc780a099ffdfe828ebd638" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Micro-block scaling：实现高效模型压缩</strong></span></span></p></div><div id="https://www.notion.so/221a0479abc7802dbeebd3ccd5c80254" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">NVFP4 的另一项核心特性是其</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">块浮点表示</strong></span><span class="SemanticString">方式，即多个数值组成的微块共享一个缩放因子。相比 MXFP4 每块包含 32 个值的设计，NVFP4 将微块大小减半至每块 16 个值，从而实现了更细粒度的缩放控制。</span></span></p></div><div id="https://www.notion.so/221a0479abc780049c4adfd34f775e73" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在 AI 模型中，大型张量常常包含混合的大值与小值，若采用统一的“全局”缩放策略，容易造成严重的量化误差，从而影响模型性能。而 NVFP4 更紧凑的分组方式提供了两倍的缩放匹配机会，更好地适应局部动态范围，显著降低这类误差。</span></span></p></div><div id="https://www.notion.so/221a0479abc780ffa629f8f9cdc8c105" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">要更清楚地理解 NVFP4 如何提升量化精度，可以将其与前代格式 MXFP4 直接对比。两者都采用数值分组并共享缩放因子的方式，但 NVFP4 的关键创新在于更小的块尺寸与更强的缩放适应性。通过将块大小从 32 减至 16，NVFP4 可更局部地对数据动态范围进行调整，这有助于保留模型权重或激活值中那些微小但关键的差异。图 5 展示了该机制在实际中的运作方式。</span></span></p></div><div id="https://www.notion.so/221a0479abc7807ca4fad4b5ac311d14" class="Image Image--PageWidth"><figure><a href="attachment:d6b55080-5552-4a30-8928-fd5d0cd2ba0c:image.png?width=809.96875"><img src="attachment:d6b55080-5552-4a30-8928-fd5d0cd2ba0c:image.png?width=809.96875" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/221a0479abc7805590abdedcd754a082" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">工作机制详解</strong></span></span></p></div><div id="https://www.notion.so/221a0479abc78003a774de7c9bb360d7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在每个 16 个值组成的微块中，每个 4 位编码值 xqx_q（范围为 -6 到 +6）都会经过如下缩放转换：</span></span></p></div><p id="https://www.notion.so/221a0479abc780ae8433f68421421072" class="Equation" data-latex="⁍"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>⁍</mtext></mrow><annotation encoding="application/x-tex">⁍</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mord">⁍</span></span></span></span></span></p><div id="https://www.notion.so/221a0479abc780968510db8a5bbe4f26" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">其中，ss 是一个动态计算的高精度 FP8（E4M3）缩放因子，其值被选取以</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">最小化整个微块的量化误差</strong></span><span class="SemanticString">。通过对每组 16 个元素重新计算 ss，NVFP4 能在保持 4 位精度的同时有效降低量化误差，同时相比更高精度格式显著减少内存占用与计算复杂度。</span></span></p></div><div id="https://www.notion.so/221a0479abc7807c88cee90a3ea11437" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这种结构设计使得 NVFP4 不仅是一种低精度格式，更是在</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">模型智能保持方面迈出重要一步</strong></span><span class="SemanticString">的压缩技术。</span></span></p></div><h1 id="https://www.notion.so/221a0479abc780f08ec0f6aa64e632e8" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/221a0479abc780f08ec0f6aa64e632e8"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">NVFP4 与 FP8：模型性能与内存效率对比</strong></span></span></h1><div id="https://www.notion.so/221a0479abc78091b335e519dcca9ab1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">量化带来的优势主要体现在两个方面：</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">减轻内存负担</strong></span><span class="SemanticString">与</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">简化计算操作</strong></span><span class="SemanticString">。这两点共同缓解了对内存带宽的压力，从而提升输出 token 的吞吐率。此外，量化还能缩短端到端推理时延，尤其是在简化注意力层计算时，其带来的直接性能提升在 prefill 阶段尤为明显。有关这些指标及其对整体推理性能的贡献，可参考《LLM Inference Benchmarking: Fundamental Concepts》。</span></span></p></div><h3 id="https://www.notion.so/221a0479abc7809e9067fedfc62c1e23" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/221a0479abc7809e9067fedfc62c1e23"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">模型性能</span></span></h3><div id="https://www.notion.so/221a0479abc7809d8324ea749503ba1e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在推理优化中，保持模型智能至关重要，而这正是 NVFP4 所设计要解决的平衡问题。NVFP4 的承诺在于：</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">以 4 位量化实现显著推理性能提升的同时，尽可能减少对模型精度的影响</strong></span><span class="SemanticString">。图 6 展示了 DeepSeek-R1-0528 模型在七项评估任务中的精度对比，结果显示 FP8 与 NVFP4 量化版本之间的精度差异极小，验证了 NVFP4 具备高效量化同时保留模型性能的能力。</span></span></p></div><div id="https://www.notion.so/221a0479abc78048a5b0f0acb53a1fae" class="Image Image--PageWidth"><figure><a href="attachment:f734f049-dda1-4987-b5bc-1050a2a40fb0:image.png?width=709.96875"><img src="attachment:f734f049-dda1-4987-b5bc-1050a2a40fb0:image.png?width=709.96875" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/221a0479abc78077b238f43b5d77444f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">该分析表明，在对 DeepSeek-R1-0528 模型从原始 FP8 格式使用后训练量化（PTQ）转换为 NVFP4 后，其在关键语言建模任务中的精度下降不超过 1%。在 AIME 2024 任务中，NVFP4 的精度甚至比 FP8 高出 2%。</span></span></p></div><h3 id="https://www.notion.so/221a0479abc7801eb7dac5793ee813c5" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/221a0479abc7801eb7dac5793ee813c5"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">内存效率</span></span></h3><div id="https://www.notion.so/221a0479abc7803e9b36fbe4fefa6f13" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">FP8 已在 Hopper 与 Blackwell 架构中广泛支持，并相较于此前最小的 16 位浮点格式（如 FP16/BF16），在内存与时延/吞吐方面带来显著优势。现在，NVFP4 为 Blackwell 架构上的 AI 负载提供了一种</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">更紧凑且精度可控的数据格式</strong></span><span class="SemanticString">。</span></span></p></div><div id="https://www.notion.so/221a0479abc780da9aa5dbabadd327dd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">NVFP4 每存储一个值仅占 4 位（外加轻量开销：每 16 个值共享一个 FP8 缩放因子，折合约 4.5 位/值，外加每张量一个 FP32 的二级缩放因子），</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">相较于 FP16 内存占用减少约 3.5 倍，相较 FP8 也减少约 1.8 倍</strong></span><span class="SemanticString">。</span></span></p></div><div id="https://www.notion.so/221a0479abc780c7aa82fcfae659f312" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">当这一效率提升扩展到 NVIDIA GB300 NVL72 架构时，其优势更加显著：每台系统包含 36 颗 Grace Blackwell Ultra 超级芯片（每颗含 1 个 NVIDIA Grace CPU 与 2 个 Blackwell Ultra GPU），</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">系统总内存可达 40 TB</strong></span><span class="SemanticString">。这种高带宽内存（HBM）与 Grace 主存结合 NVFP4 在精度与容量方面的优势，为大规模 AI 推理部署带来显著益处，特别是在应对推理时的测试规模扩展挑战时表现尤为突出。</span></span></p></div><h1 id="https://www.notion.so/221a0479abc780c2b44be2ddf3e703da" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/221a0479abc780c2b44be2ddf3e703da"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">FP4 能效表现</span></span></h1><div id="https://www.notion.so/221a0479abc78059bcebcab908dfb2e9" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">降低数据精度不仅能加快推理速度、减少内存占用，还能</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">显著提升单位功耗下的性能（performance per watt）</strong></span><span class="SemanticString">。与高精度数据类型相比，4 位操作在数据移动和算术计算上所需能量更低，从而提升整体能效。</span></span></p></div><div id="https://www.notion.so/221a0479abc7801a8e36c6384288e5a1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">得益于液冷技术与 Blackwell 张量核心架构中对 FP4 的原生支持，Blackwell 和 Blackwell Ultra 在能效方面实现了重大突破：</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">相较于 NVIDIA H100 张量核心，Blackwell 的能效提升高达 25 倍，Blackwell Ultra 更是达到 50 倍</strong></span><span class="SemanticString">（详见图 7）。这些提升使得 FP4 成为构建高效、可扩展 AI 推理系统的重要技术基础。</span></span></p></div><div id="https://www.notion.so/221a0479abc78006b006d7792d1a0435" class="Image Image--PageWidth"><figure><a href="attachment:94ce025d-3e71-4660-937e-9ea84f891877:image.png?width=809.96875"><img src="attachment:94ce025d-3e71-4660-937e-9ea84f891877:image.png?width=809.96875" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h1 id="https://www.notion.so/221a0479abc78021b59ce373d9580f7c" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/221a0479abc78021b59ce373d9580f7c"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">开始使用 NVFP4</span></span></h1><div id="https://www.notion.so/221a0479abc78067a93fd3274cbb1df6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">为满足 AI 日益增长的推理需求，</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">推理生态系统正在快速拥抱 NVFP4 精度</strong></span><span class="SemanticString">。如果你希望将模型量化为 NVFP4，NVIDIA 提供了便捷的工具链——</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">TensorRT Model Optimizer 与 LLM Compressor</strong></span><span class="SemanticString">，可轻松实现 PTQ（后训练量化）、QAT（训练中量化）及其他先进量化方法。</span></span></p></div><div id="https://www.notion.so/221a0479abc780cc8e7ff53cc3a3edcd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">完成量化后，NVFP4 模型可导出为</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">统一 Hugging Face Checkpoint 格式</strong></span><span class="SemanticString">，并部署于支持 NVFP4 的推理框架中，如 </span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">TensorRT-LLM 和 vLLM</strong></span><span class="SemanticString">（SGLang 的支持即将上线）。这些框架正成为日益壮大的 NVFP4 推理生态的重要组成部分。TensorRT Model Optimizer 还支持非大语言模型（non-LLM）的量化以及导出为 ONNX 格式。</span></span></p></div><div id="https://www.notion.so/221a0479abc78070ad6be3371b3b099f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">更便利的是，你无需从零开始。</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Hugging Face 已托管多种预量化的 NVFP4 模型 checkpoint</strong></span><span class="SemanticString">，可即刻部署使用，包括 DeepSeek-R1-0528、Llama 3 和 FLUX.1-dev 等主流模型。</span></span></p></div><div id="https://www.notion.so/221a0479abc78072a9a4ee8645853ebb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">无论是从头优化还是直接使用预量化模型，NVFP4 在真实部署场景中正迅速普及，更多教程与代码示例也即将上线，敬请关注。</span></span></p></div></article>
  <footer class="Footer">
  <div>&copy; Vinci’s Garden 2024</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>

</body>

</html>